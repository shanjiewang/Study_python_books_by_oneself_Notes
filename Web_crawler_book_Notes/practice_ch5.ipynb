{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604be9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "url='https://deepmind.com.tw'\n",
    "response=requests.get(url)\n",
    "t=response.text #查看網頁內容\n",
    "print(type(t))\n",
    "# 將html建立BeautifulSoup物件\n",
    "obj_soup=bs4.BeautifulSoup(t,'lxml')\n",
    "print(type(obj_soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本Html文件解析\n",
    "\n",
    "url='myhtml.html'\n",
    "response=open(url,encoding='utf-8')\n",
    "obj_soup=bs4.BeautifulSoup(response,'lxml')\n",
    "print('列出title標籤及內容:\\n',obj_soup.title)\n",
    "print('\\n列出section標籤及內容:\\n',obj_soup.section)\n",
    "print('\\n列出標籤的內容就好:\\n',obj_soup.title.text)\n",
    "\n",
    "# find()找尋第一個符合標籤\n",
    "find_h1=obj_soup.find('h1')\n",
    "print('\\n找尋第一個h1標籤:\\n',find_h1)\n",
    "print('\\nstring和text一樣嗎??:',find_h1.string==find_h1.text)\n",
    "\n",
    "# find_all()找尋所有符合標籤\n",
    "find_all_h1=obj_soup.find_all('h1',limit=2)  # limit=要找幾個標籤節點\n",
    "print(len(find_all_h1))\n",
    "for find in find_all_h1:\n",
    "    print('\\n找尋全部h1標籤:\\n',find)\n",
    "    print('找尋全部h1標籤純內容:\\n',find.text)\n",
    "    print('使用getText()找尋全部h1標籤純內容:\\n',find.getText())\n",
    "    \n",
    "# find_all(id='')找尋所有符合id的標籤\n",
    "print('\\n找尋全部符合id的純內容:')\n",
    "find_all_id=obj_soup.find_all(id='content')  # limit=要找幾個標籤節點\n",
    "for find in find_all_id:\n",
    "    print(find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import bs4\n",
    "import re\n",
    "\n",
    "response = \"<div book-info='deepmind'>深智數位</div><h1 class='this'>AI數位轉型</h1><h1 class='those these'>不在p裡的文字<p>我不知道</p><p class='this'>資料工程師</p></h1>\"\n",
    "obj=bs4.BeautifulSoup(response,'lxml')\n",
    "\n",
    "# 遇到'data-*'含'-'之類的屬性 要使用attrs參數 相當於將數透過字典方式傳給attrs\n",
    "print('遇到\"data-*\"含\"-\"之類的屬性,要使用attrs參數,相當於將數透過字典方式傳給attrs:')\n",
    "find_attr=obj.find(attrs={'book-info':'deepmind'})\n",
    "print(find_attr.text)\n",
    "\n",
    "print('\\n找尋標籤內class的方法:')\n",
    "find_class=obj.find('h1',class_='this')\n",
    "print(find_class)\n",
    "# 直接寫也是找class\n",
    "find_class=obj.find('h1','this')\n",
    "print(find_class.text)\n",
    "\n",
    "print('\\n用re找到class內有符合部分字串就秀出:')\n",
    "re1=obj.find('h1',class_=re.compile('ose'))\n",
    "print(re1)\n",
    "\n",
    "print('\\n一個屬性比對成就算成功:')\n",
    "find_one=obj.find('h1',class_='those')\n",
    "print(find_one)\n",
    "\n",
    "print('\\nselect可以一次尋找所有相符的元素 找p:')\n",
    "find_p=obj.select('p')\n",
    "print(find_p)\n",
    "\n",
    "print('\\nselect可以一次尋找所有相符的元素 找class=this:')\n",
    "find_this=obj.select('.this')\n",
    "print(find_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a668c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select可以一次尋找所有相符的元素.並透過list回傳\n",
    "\n",
    "import bs4\n",
    "\n",
    "print('因為回傳是list 要找list單一項目要給值:')\n",
    "url='myhtml.html'\n",
    "response=open(url,encoding='utf-8')\n",
    "obj=bs4.BeautifulSoup(response,'lxml')\n",
    "find_content=obj.select('#content')\n",
    "print(find_content)\n",
    "print(find_content[0].getText())\n",
    "print(str(find_content[0].getText()))\n",
    "\n",
    "print('\\n結果相同,型態不同,可使用的方法也不同:')\n",
    "print(type(find_content[0]))\n",
    "print(type(str(find_content[0])))\n",
    "\n",
    "print('\\n將attrs應用在list元素 列出dict結果:')\n",
    "print(find_content[0].attrs)\n",
    "\n",
    "print('\\n搜尋p 印出含子標籤與不含的結果:')\n",
    "find_p=obj.select('p')\n",
    "print(len(find_p))\n",
    "for p in find_p:\n",
    "    print(str(p))  # 含子標籤\n",
    "    print(p.getText())   # 不含子標籤\n",
    "    print(p.text)  # 不含子標籤\n",
    "    \n",
    "print('\\n用get()找尋標籤字串的img:')\n",
    "find_img=obj.select('img')\n",
    "print(find_img)\n",
    "print(len(find_img))\n",
    "\n",
    "for img in find_img:\n",
    "    print(img)\n",
    "    print(img.get('src'))  # 取得圖檔src標籤字串的屬性內容\n",
    "    print(img['src'])   # 取得圖檔src標籤字串的屬性內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40c87ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body><p>﻿<!DOCTYPE html>\n",
      "\n",
      "</p>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<title>ch5_2_1.html</title>\n",
      "<h1>台灣旅遊景點排名</h1>\n",
      "<ol type=\"a\">\n",
      "<li>故宮博物院</li><li>日月潭</li><li>阿里山</li>\n",
      "</ol>\n",
      "<h2>台灣夜市排名</h2>\n",
      "<ol type=\"A\">\n",
      "<li>士林夜市</li><li>永康夜市</li><li>逢甲夜市</li>\n",
      "</ol>\n",
      "<h2>台灣人口排名</h2>\n",
      "<ol type=\"i\">\n",
      "<li>新北市</li><li>台北市</li><li>桃園市</li>\n",
      "</ol>\n",
      "<h2>台灣最健康大學排名</h2>\n",
      "<ol type=\"I\">\n",
      "<li>明志科大</li><li>台灣體院</li><li>台北體院</li>\n",
      "</ol>\n",
      "</body></html> \n",
      "\n",
      "[<h2>台灣夜市排名</h2>, <h2>台灣人口排名</h2>, <h2>台灣最健康大學排名</h2>] \n",
      "\n",
      "台灣最健康大學排名\n",
      "\n",
      "找尋type=\"I\"的ol: <ol type=\"I\">\n",
      "<li>明志科大</li><li>台灣體院</li><li>台北體院</li>\n",
      "</ol>\n",
      "\n",
      "找尋li: [<li>明志科大</li>, <li>台灣體院</li>, <li>台北體院</li>]\n",
      "明志科大\n",
      "台灣體院\n",
      "台北體院\n",
      "['明志科大', '台灣體院', '台北體院']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>台灣最健康大學排名</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>明志科大</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>台灣體院</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>台北體院</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  台灣最健康大學排名\n",
       "1      明志科大\n",
       "2      台灣體院\n",
       "3      台北體院"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4,requests\n",
    "import pandas as pd\n",
    "\n",
    "url='ch5_2_1.html'\n",
    "response=open(url,encoding='utf-8')\n",
    "obj=bs4.BeautifulSoup(response,'lxml')\n",
    "li_list=[]\n",
    "print(obj,'\\n')\n",
    "find_h2=obj.find_all('h2')\n",
    "print(find_h2,'\\n')\n",
    "ff=find_h2[2].text\n",
    "print(ff)\n",
    "find_ol=obj.find('ol',type='I')\n",
    "print('\\n找尋type=\"I\"的ol:',find_ol)\n",
    "find_li=find_ol.find_all('li')\n",
    "print('\\n找尋li:',find_li)\n",
    "for li in find_li:\n",
    "    print(li.text)\n",
    "    li_list.append(li.text)\n",
    "print(li_list)\n",
    "\n",
    "# 轉成DataFrame並儲存csv\n",
    "pp=pd.DataFrame(li_list,columns=[ff],index=range(1,len(li_list)+1))\n",
    "pp.to_csv('ch5_2_1.csv')\n",
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "98576613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "標題:\n",
      "[<h1>台灣旅遊景點排名</h1>]\n",
      "[<h2>台灣夜市排名</h2>, <h2>台灣人口排名</h2>, <h2>台灣最健康大學排名</h2>]\n",
      "['台灣旅遊景點', '台灣夜市', '台灣人口', '台灣最健康大學']\n",
      "\n",
      "內容:\n",
      "\n",
      "故宮博物院日月潭阿里山\n",
      "\n",
      "\n",
      "士林夜市永康夜市逢甲夜市\n",
      "\n",
      "\n",
      "新北市台北市桃園市\n",
      "\n",
      "\n",
      "明志科大台灣體院台北體院\n",
      "\n",
      "把li項目建成同一個list: ['故宮博物院', '日月潭', '阿里山', '士林夜市', '永康夜市', '逢甲夜市', '新北市', '台北市', '桃園市', '明志科大', '台灣體院', '台北體院']\n",
      "0\n",
      "3\n",
      "6\n",
      "9\n",
      "轉成雙陣列比較方便轉DataFrame: [['故宮博物院', '日月潭', '阿里山'], ['士林夜市', '永康夜市', '逢甲夜市'], ['新北市', '台北市', '桃園市'], ['明志科大', '台灣體院', '台北體院']]\n",
      "\n",
      "需要行列交換:\n",
      "              1     2     3\n",
      "台灣旅遊景點   故宮博物院   日月潭   阿里山\n",
      "台灣夜市      士林夜市  永康夜市  逢甲夜市\n",
      "台灣人口       新北市   台北市   桃園市\n",
      "台灣最健康大學   明志科大  台灣體院  台北體院\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>台灣旅遊景點</th>\n",
       "      <th>台灣夜市</th>\n",
       "      <th>台灣人口</th>\n",
       "      <th>台灣最健康大學</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>故宮博物院</td>\n",
       "      <td>士林夜市</td>\n",
       "      <td>新北市</td>\n",
       "      <td>明志科大</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>日月潭</td>\n",
       "      <td>永康夜市</td>\n",
       "      <td>台北市</td>\n",
       "      <td>台灣體院</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>阿里山</td>\n",
       "      <td>逢甲夜市</td>\n",
       "      <td>桃園市</td>\n",
       "      <td>台北體院</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  台灣旅遊景點  台灣夜市 台灣人口 台灣最健康大學\n",
       "1  故宮博物院  士林夜市  新北市    明志科大\n",
       "2    日月潭  永康夜市  台北市    台灣體院\n",
       "3    阿里山  逢甲夜市  桃園市    台北體院"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自己做 用find_all把全部爬下來存成csv 太複雜版\n",
    "import bs4,requests\n",
    "import pandas as pd\n",
    "\n",
    "url='ch5_2_1.html'\n",
    "response=open(url,encoding='utf-8')\n",
    "obj=bs4.BeautifulSoup(response,'lxml')\n",
    "\n",
    "print('標題:')\n",
    "find_h1_list=[]\n",
    "find_h1=obj.find_all('h1')\n",
    "find_h2=obj.find_all('h2')\n",
    "print(find_h1)\n",
    "print(find_h2)\n",
    "find_h1.extend(find_h2)\n",
    "for i in find_h1:\n",
    "    t=i.text.replace('排名','')\n",
    "    find_h1_list.append(t)\n",
    "print(find_h1_list)\n",
    "\n",
    "print('\\n內容:')\n",
    "ol_list,li_list=[],[]\n",
    "find_ol=obj.find_all('ol')\n",
    "for ol in range(0,len(find_ol)):\n",
    "    print(find_ol[ol].text)\n",
    "    find_li=find_ol[ol].find_all('li')\n",
    "    for li in range(0,len(find_li)):\n",
    "#         print(ol)\n",
    "#         print(find_li[li].text)\n",
    "        li_list.append(find_li[li].text)\n",
    "print('把li項目建成同一個list:',li_list)\n",
    "\n",
    "for j in range(0,len(li_list),len(find_li)):\n",
    "    print(j)\n",
    "    ol_list.append(li_list[j:j+len(find_li)])\n",
    "print('轉成雙陣列比較方便轉DataFrame:',ol_list)\n",
    "\n",
    "ff=pd.DataFrame(ol_list,index=find_h1_list,columns=range(1,len(find_li)+1))\n",
    "print('\\n需要行列交換:\\n',ff)\n",
    "ff=ff.T   # DataFrame行列交換\n",
    "ff.to_csv('ch5_2_1_ALL.csv')\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0a86f41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "標題:\n",
      "[<h1>台灣旅遊景點排名</h1>]\n",
      "[<h2>台灣夜市排名</h2>, <h2>台灣人口排名</h2>, <h2>台灣最健康大學排名</h2>]\n",
      "['台灣旅遊景點', '台灣夜市', '台灣人口', '台灣最健康大學']\n",
      "\n",
      "內容:\n",
      "\n",
      "故宮博物院日月潭阿里山\n",
      "\n",
      "ol: 0\n",
      "\n",
      "士林夜市永康夜市逢甲夜市\n",
      "\n",
      "ol: 1\n",
      "\n",
      "新北市台北市桃園市\n",
      "\n",
      "ol: 2\n",
      "\n",
      "明志科大台灣體院台北體院\n",
      "\n",
      "ol: 3\n",
      "[['故宮博物院', '日月潭', '阿里山'], ['士林夜市', '永康夜市', '逢甲夜市'], ['新北市', '台北市', '桃園市'], ['明志科大', '台灣體院', '台北體院']]\n",
      "\n",
      "需要行列交換:\n",
      "              1     2     3\n",
      "台灣旅遊景點   故宮博物院   日月潭   阿里山\n",
      "台灣夜市      士林夜市  永康夜市  逢甲夜市\n",
      "台灣人口       新北市   台北市   桃園市\n",
      "台灣最健康大學   明志科大  台灣體院  台北體院\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>台灣旅遊景點</th>\n",
       "      <th>台灣夜市</th>\n",
       "      <th>台灣人口</th>\n",
       "      <th>台灣最健康大學</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>故宮博物院</td>\n",
       "      <td>士林夜市</td>\n",
       "      <td>新北市</td>\n",
       "      <td>明志科大</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>日月潭</td>\n",
       "      <td>永康夜市</td>\n",
       "      <td>台北市</td>\n",
       "      <td>台灣體院</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>阿里山</td>\n",
       "      <td>逢甲夜市</td>\n",
       "      <td>桃園市</td>\n",
       "      <td>台北體院</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  台灣旅遊景點  台灣夜市 台灣人口 台灣最健康大學\n",
       "1  故宮博物院  士林夜市  新北市    明志科大\n",
       "2    日月潭  永康夜市  台北市    台灣體院\n",
       "3    阿里山  逢甲夜市  桃園市    台北體院"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自己做 用find_all把全部爬下來存成csv 簡潔版\n",
    "import bs4,requests\n",
    "import pandas as pd\n",
    "\n",
    "url='ch5_2_1.html'\n",
    "response=open(url,encoding='utf-8')\n",
    "obj=bs4.BeautifulSoup(response,'lxml')\n",
    "\n",
    "print('標題:')\n",
    "find_h1_list=[]\n",
    "find_h1=obj.find_all('h1')\n",
    "find_h2=obj.find_all('h2')\n",
    "print(find_h1)\n",
    "print(find_h2)\n",
    "find_h1.extend(find_h2)\n",
    "for i in find_h1:\n",
    "    t=i.text.replace('排名','')\n",
    "    find_h1_list.append(t)\n",
    "print(find_h1_list)\n",
    "\n",
    "print('\\n內容:')\n",
    "\n",
    "lists1,lists2,lists3,lists4=[],[],[],[]\n",
    "big_lists=[lists1,lists2,lists3,lists4]\n",
    "# for lists in range(0,len(find_h1)):\n",
    "#     ff+lists=[]\n",
    "#     print(ff)\n",
    "    \n",
    "find_ol=obj.find_all('ol')\n",
    "for ol in range(0,len(find_ol)):\n",
    "    print(find_ol[ol].text)\n",
    "    print('ol:',ol)\n",
    "    find_li=find_ol[ol].find_all('li')\n",
    "    # 直接變雙陣列\n",
    "    for li in range(0,len(find_li)):\n",
    "        big_lists[ol].append(find_li[li].text)\n",
    "print(big_lists)\n",
    "\n",
    "ff=pd.DataFrame(ol_list,index=find_h1_list,columns=range(1,len(find_li)+1))\n",
    "print('\\n需要行列交換:\\n',ff)\n",
    "ff=ff.T   # DataFrame行列交換\n",
    "ff.to_csv('ch5_2_1_ALL.csv')\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b3100c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原本節點:\n",
      " 日本首都 \n",
      "parent()上移至父節點:\n",
      " [<dt>Washington</dt>, <dd>美國首都</dd>, <dt>Tokyo</dt>, <dd>日本首都</dd>, <dt>Paris</dt>, <dd>法國首都</dd>]\n",
      "\n",
      "國家: ['美國首都', '日本首都', '法國首都']\n",
      "首都: ['Washington', 'Tokyo', 'Paris']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'美國首都': 'Washington', '日本首都': 'Tokyo', '法國首都': 'Paris'}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 爬蟲並以dict方式列出國家\n",
    "\n",
    "url='ch5_2_2.html'\n",
    "dt_list,dd_list=[],[]\n",
    "response=open(url,encoding='utf-8')\n",
    "obj=bs4.BeautifulSoup(response,'lxml')\n",
    "find_dd=obj.find_all('dd')\n",
    "find_dt=obj.find_all('dt')\n",
    "\n",
    "# parent()上移至父節點\n",
    "dd=find_dd[1].parent()\n",
    "print('原本節點:\\n',find_dd[1].text,'\\nparent()上移至父節點:\\n',dd)\n",
    "\n",
    "# 必須先把有標籤的list用for轉成沒標籤的list\n",
    "for dd in find_dd:\n",
    "    dd_list.append(dd.text)\n",
    "for dt in find_dt:\n",
    "    dt_list.append(dt.text)\n",
    "    \n",
    "print('\\n國家:',dd_list)\n",
    "print('首都:',dt_list)\n",
    "\n",
    "# 用zip將list轉字典\n",
    "dict_list=dict(zip(dd_list,dt_list))\n",
    "dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "844036e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tr><td>長江</td><td>中國</td><td>亞洲</td></tr>, <tr><td>尼羅河</td><td>埃及</td><td>非洲</td></tr>, <tr><td>亞馬遜河</td><td>巴西</td><td>南美洲</td></tr>]\n",
      "\n",
      "國家: ['中國', '埃及', '巴西']\n",
      "河川: ['長江', '尼羅河', '亞馬遜河']\n",
      "{'中國': '長江', '埃及': '尼羅河', '巴西': '亞馬遜河'}\n"
     ]
    }
   ],
   "source": [
    "# 爬取表格文件 使用find_all\n",
    "\n",
    "import requests,bs4\n",
    "\n",
    "river_list,country_list=[],[]\n",
    "url='ch5_2_3.html'\n",
    "response=open(url,encoding='utf-8')\n",
    "obj=bs4.BeautifulSoup(response, 'lxml')\n",
    "find_table=obj.find('table').find('tbody')\n",
    "find_tr=find_table.find_all('tr')\n",
    "print(find_tr)\n",
    "\n",
    "# find只會找出第一個 直接用find_all就可\n",
    "for tr in find_tr:\n",
    "    find_td=tr.find_all('td')\n",
    "    country_list.append(find_td[1].text)\n",
    "    river_list.append(find_td[0].text)\n",
    "print('\\n國家:',country_list)\n",
    "print('河川:',river_list)\n",
    "dicts=dict(zip(country_list,river_list))\n",
    "print(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c63fcb65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tr><td>長江</td><td>中國</td><td>亞洲</td></tr>, <tr><td>尼羅河</td><td>埃及</td><td>非洲</td></tr>, <tr><td>亞馬遜河</td><td>巴西</td><td>南美洲</td></tr>]\n",
      "\n",
      "國家: ['中國', '埃及', '巴西']\n",
      "河川: ['長江', '尼羅河', '亞馬遜河']\n",
      "洲名: ['亞洲', '非洲', '南美洲']\n",
      "{'亞洲': '長江', '非洲': '尼羅河', '南美洲': '亞馬遜河'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['中國', '長江', '亞洲']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 爬取表格文件 使用find_next_sibling()及find_previous_sibling() 上下個節點\n",
    "\n",
    "import requests,bs4\n",
    "\n",
    "river_list,country_list,island_list=[],[],[]\n",
    "url='ch5_2_3.html'\n",
    "response=open(url,encoding='utf-8')\n",
    "obj=bs4.BeautifulSoup(response, 'lxml')\n",
    "find_table=obj.find('table').find('tbody')\n",
    "find_tr=find_table.find_all('tr')\n",
    "print(find_tr)\n",
    "\n",
    "# find只會找出第一個\n",
    "for tr in find_tr:\n",
    "    find_td=tr.find_all('td')\n",
    "    country_list.append(find_td[1].text)\n",
    "    river=find_td[1].find_previous_sibling('td')  # 尋找同父節點的上一個相同層級的節點\n",
    "    river_list.append(river.text)\n",
    "    island=find_td[1].find_next_sibling('td')   # 尋找同父節點的下一個相同層級的節點\n",
    "    island_list.append(island.text)\n",
    "print('\\n國家:',country_list)\n",
    "print('河川:',river_list)\n",
    "print('洲名:',island_list)\n",
    "dicts=dict(zip(island_list,river_list))\n",
    "print(dicts)\n",
    "aa=[]\n",
    "aa=[country_list[0],river_list[0],island_list[0]]\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b32b74aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tr><td>長江</td><td>中國</td><td>亞洲</td></tr>, <tr><td>尼羅河</td><td>埃及</td><td>非洲</td></tr>, <tr><td>亞馬遜河</td><td>巴西</td><td>南美洲</td></tr>] \n",
      "\n",
      "0 _ [<td>長江</td>, <td>中國</td>, <td>亞洲</td>]\n",
      "td.text: 長江\n",
      "[<td>長江</td>, <td>中國</td>, <td>亞洲</td>]\n",
      "td.text: 中國\n",
      "[<td>長江</td>, <td>中國</td>, <td>亞洲</td>]\n",
      "td.text: 亞洲\n",
      "[<td>長江</td>, <td>中國</td>, <td>亞洲</td>]\n",
      "中國\n",
      "1 _ [<td>尼羅河</td>, <td>埃及</td>, <td>非洲</td>]\n",
      "td.text: 尼羅河\n",
      "[<td>尼羅河</td>, <td>埃及</td>, <td>非洲</td>]\n",
      "td.text: 埃及\n",
      "[<td>尼羅河</td>, <td>埃及</td>, <td>非洲</td>]\n",
      "td.text: 非洲\n",
      "[<td>尼羅河</td>, <td>埃及</td>, <td>非洲</td>]\n",
      "埃及\n",
      "2 _ [<td>亞馬遜河</td>, <td>巴西</td>, <td>南美洲</td>]\n",
      "td.text: 亞馬遜河\n",
      "[<td>亞馬遜河</td>, <td>巴西</td>, <td>南美洲</td>]\n",
      "td.text: 巴西\n",
      "[<td>亞馬遜河</td>, <td>巴西</td>, <td>南美洲</td>]\n",
      "td.text: 南美洲\n",
      "[<td>亞馬遜河</td>, <td>巴西</td>, <td>南美洲</td>]\n",
      "巴西\n",
      "\n",
      "把指定標籤設為字典鍵: ['中國', '埃及', '巴西']\n",
      "\n",
      "雙值字典成功: {'中國': ['長江', '中國', '亞洲'], '埃及': ['尼羅河', '埃及', '非洲'], '巴西': ['亞馬遜河', '巴西', '南美洲']}\n"
     ]
    }
   ],
   "source": [
    "# 自己做 爬取表格文件 轉成雙值字典\n",
    "\n",
    "import requests,bs4\n",
    "\n",
    "china_list,africa_list,brazil_list=[],[],[]\n",
    "keys=[]\n",
    "big_dicts=[china_list,africa_list,brazil_list]\n",
    "url='ch5_2_3.html'\n",
    "response=open(url,encoding='utf-8')\n",
    "obj=bs4.BeautifulSoup(response, 'lxml')\n",
    "find_table=obj.find('table').find('tbody')\n",
    "find_tr=find_table.find_all('tr')\n",
    "print(find_tr,'\\n')\n",
    "\n",
    "for tr in range(len(find_tr)):\n",
    "    find_td=find_tr[tr].find_all('td')\n",
    "    print(tr,'_',find_td)\n",
    "    for td in find_td:\n",
    "        print('td.text:',td.text)\n",
    "        # 去除標籤後 依序加入list\n",
    "        big_dicts[tr].append(td.text)\n",
    "    print(big_dicts[tr][1])\n",
    "    # 把指定標籤設為字典鍵\n",
    "    keys.append(big_dicts[tr][1])\n",
    "print('\\n把指定標籤設為字典鍵:',keys)\n",
    "\n",
    "dicts=dict(zip(keys,big_dicts))\n",
    "print('\\n雙值字典成功:',dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "aefc31f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用find_next_siblings()及find_previous_siblings() 所有節點:\n",
      "<h2>台灣夜市排名</h2>\n",
      "[<h2>台灣人口排名</h2>, <h2>台灣最健康大學排名</h2>]\n",
      "[<h2>台灣人口排名</h2>, <h2>台灣夜市排名</h2>]\n",
      "\n",
      "將parent()與所有節點一起用:\n",
      "<td>長江</td>\n",
      "\n",
      ".parent.find_next_siblings()回到父節點並秀出之後的所有節點:\n",
      "[<tr><td>尼羅河</td><td>埃及</td><td>非洲</td></tr>, <tr><td>亞馬遜河</td><td>巴西</td><td>南美洲</td></tr>]\n",
      "\n",
      ".parent.find_next_siblings()回到父節點並秀出之前的所有節點:\n",
      "[<tr><td>尼羅河</td><td>埃及</td><td>非洲</td></tr>, <tr><td>長江</td><td>中國</td><td>亞洲</td></tr>]\n"
     ]
    }
   ],
   "source": [
    "# 爬取表格文件 使用find_next_siblings()及find_previous_siblings() 所有節點\n",
    "print('使用find_next_siblings()及find_previous_siblings() 所有節點:')\n",
    "import bs4,requests\n",
    "\n",
    "url='ch5_2_1.html'\n",
    "url_3='ch5_2_3.html'\n",
    "response=open(url,encoding='utf-8')\n",
    "obj=bs4.BeautifulSoup(response,'lxml')\n",
    "find_1=obj.find('h2')\n",
    "\n",
    "# 爬取h2之後的所有節點 會一起印出\n",
    "find_2=find_1.find_next_siblings('h2')\n",
    "print(find_1)\n",
    "print(find_2)\n",
    "\n",
    "find_3=obj.find_all('h2')\n",
    "# 爬取h2之前的所有節點 會一起印出\n",
    "find_4=find_3[2].find_previous_siblings('h2')\n",
    "print(find_4)\n",
    "\n",
    "print('\\n將parent()與所有節點一起用:')\n",
    "response_3=open(url_3,encoding='utf-8')\n",
    "obj_3=bs4.BeautifulSoup(response_3,'lxml')\n",
    "find_5=obj_3.find('table').find('tbody')\n",
    "find_6=find_5.find_all('tr')\n",
    "find_7=find_6[0].find('td')  # 使用find()找到第一個\n",
    "print(find_7)\n",
    "\n",
    "print('\\n.parent.find_next_siblings()回到父節點並秀出之後的所有節點:')\n",
    "find_8=find_7.parent.find_next_siblings()\n",
    "print(find_8)\n",
    "\n",
    "print('\\n.parent.find_next_siblings()回到父節點並秀出之前的所有節點:')\n",
    "find_7=find_6[2].find_all('td')  # 使用find_all()找到全部所以要再切list\n",
    "find_9=find_7[2].parent.find_previous_siblings()\n",
    "print(find_9)\n",
    "\n",
    "# find_9=find_8.find_next_sibling('tr')\n",
    "# print(find_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "79b77418",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "網頁載入中...\n",
      "載入成功!!\n",
      "讀取到 70 張圖片\n",
      "888~\n",
      "888~\n",
      "888~\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/branding.svg\n",
      "下載成功的圖片檔名: branding.svg\n",
      "888~\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default3v2.jpg\n",
      "下載成功的圖片檔名: default3v2.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "888~\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/computer/pickup_tag.png\n",
      "下載成功的圖片檔名: pickup_tag.png\n",
      "888~\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/computer/pickup_tag.png\n",
      "下載成功的圖片檔名: pickup_tag.png\n",
      "888~\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/computer/pickup_tag.png\n",
      "下載成功的圖片檔名: pickup_tag.png\n",
      "888~\n",
      "888~\n",
      "888~\n",
      "888~\n",
      "888~\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/default.jpg\n",
      "下載成功的圖片檔名: default.jpg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/default_img/branding_white_bg_no_eng.svg\n",
      "下載成功的圖片檔名: branding_white_bg_no_eng.svg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/badge/app-store.svg\n",
      "下載成功的圖片檔名: app-store.svg\n",
      "圖片下載中.. https://hiking.biji.co//resource/images/badge/play-store.png\n",
      "下載成功的圖片檔名: play-store.png\n"
     ]
    }
   ],
   "source": [
    "# 爬蟲實戰 健行筆記首頁所有圖片下載\n",
    "\n",
    "import bs4,requests,os\n",
    "\n",
    "# 要爬每個網站 最好都偽裝成瀏覽器\n",
    "headers = { 'User-Agent':'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'}\n",
    "url='https://hiking.biji.co/'\n",
    "html=requests.get(url, headers=headers)\n",
    "print('網頁載入中...')\n",
    "html.raise_for_status()\n",
    "print('載入成功!!')\n",
    "\n",
    "# 如果無此資料夾 就建一個\n",
    "img_Dir='hiking_biji_imgs'\n",
    "if os.path.exists(img_Dir)==False:\n",
    "    os.mkdir(img_Dir)\n",
    "\n",
    "obj=bs4.BeautifulSoup(html.text,'lxml')\n",
    "# 讀取到所有網頁\n",
    "# print(obj)\n",
    "\n",
    "imgs=obj.select('img')\n",
    "print('讀取到',len(imgs),'張圖片')\n",
    "if len(imgs)>0:\n",
    "    for i in range(len(imgs)):\n",
    "        img_url=imgs[i].get('src')\n",
    "        if img_url !=None and img_url[0:5]!='https':            \n",
    "            img_url=url+img_url\n",
    "            print('圖片下載中..',img_url)\n",
    "            try:\n",
    "                load_img=requests.get(img_url)\n",
    "                load_img.raise_for_status()\n",
    "                print('下載成功的圖片檔名: %s'% os.path.basename(img_url))\n",
    "                \n",
    "                # 先開啟檔案 再儲存圖片 \n",
    "                # 注意大小寫是否打錯 若顯示下載成功 卻沒存進去 應該都是打錯字\n",
    "                # os.path.join 路徑拼接\n",
    "                # os.path.basename 只取檔名不取路徑\n",
    "                img_file=open(os.path.join(img_Dir,os.path.basename(img_url)),'wb') \n",
    "                \n",
    "                # 分段存入 圖片比較不易出錯\n",
    "                for times in load_img.iter_content(10240):\n",
    "                    img_file.write(times)\n",
    "                img_file.close()\n",
    "                \n",
    "            except Exception as err:\n",
    "                continue\n",
    "                print('圖片下載失敗')\n",
    "        else:\n",
    "            print('888~')\n",
    "else:\n",
    "    print('沒有找到圖片')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f29c4e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"ball_tx ball_green\">08 </div>, <div class=\"ball_tx ball_green\">15 </div>, <div class=\"ball_tx ball_green\">12 </div>, <div class=\"ball_tx ball_green\">04 </div>, <div class=\"ball_tx ball_green\">14 </div>, <div class=\"ball_tx ball_green\">17 </div>, <div class=\"ball_tx ball_green\">04 </div>, <div class=\"ball_tx ball_green\">08 </div>, <div class=\"ball_tx ball_green\">12 </div>, <div class=\"ball_tx ball_green\">14 </div>, <div class=\"ball_tx ball_green\">15 </div>, <div class=\"ball_tx ball_green\">17 </div>, <div class=\"ball_tx ball_green\">08 </div>, <div class=\"ball_tx ball_green\">15 </div>, <div class=\"ball_tx ball_green\">12 </div>, <div class=\"ball_tx ball_green\">04 </div>, <div class=\"ball_tx ball_green\">14 </div>, <div class=\"ball_tx ball_green\">17 </div>, <div class=\"ball_tx ball_green\">04 </div>, <div class=\"ball_tx ball_green\">08 </div>, <div class=\"ball_tx ball_green\">12 </div>, <div class=\"ball_tx ball_green\">14 </div>, <div class=\"ball_tx ball_green\">15 </div>, <div class=\"ball_tx ball_green\">17 </div>]\n",
      "[<div class=\"ball_red\">03 </div>, <div class=\"ball_red\">29 </div>]\n",
      "08 \n",
      "15 \n",
      "12 \n",
      "04 \n",
      "14 \n",
      "17 \n",
      "04 \n",
      "08 \n",
      "12 \n",
      "14 \n",
      "15 \n",
      "17 \n",
      "[8, 15, 12, 4, 14, 17, 4, 8, 12, 14, 15, 17]\n",
      "開出順序: 8 15 12 4 14 17 \n",
      "大小順序: 4 8 12 14 15 17 \n",
      "第二區: 03 \n"
     ]
    }
   ],
   "source": [
    "# 使用爬蟲找出威力採最新開獎結果 使用select較快\n",
    "import bs4,requests\n",
    "\n",
    "ball_list=[]\n",
    "url='http://www.taiwanlottery.com.tw'\n",
    "html=requests.get(url)\n",
    "html.raise_for_status()\n",
    "obj=bs4.BeautifulSoup(html.text,'lxml')\n",
    "tag_1=obj.select('.contents_box02 .ball_green')\n",
    "tag_2=obj.select('.contents_box02 .ball_red')\n",
    "print(tag_1)\n",
    "print(tag_2)\n",
    "tag_len=int(len(tag_1)/2)\n",
    "for i in range(tag_len):\n",
    "    print(tag_1[i].text)\n",
    "    ball_list.append(int(tag_1[i].text))\n",
    "print(ball_list)\n",
    "print('開出順序:',end=' ')\n",
    "for i in ball_list[0:6]:\n",
    "    print(i,end=' ')\n",
    "print('\\n大小順序:',end=' ')\n",
    "for i in ball_list[6:tag_len]:\n",
    "    print(i,end=' ')\n",
    "print('\\n第二區:',tag_2[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5abc0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "馬斯克下戰帖 要向普丁單挑\n",
      "['D(b)', 'Pos(r)', 'W(100%)', 'H(100%)', 'Ov(h)', 'V(h)', 'active_V(v)']\n",
      "情勢翻轉 留學生搶台灣職缺\n",
      "['D(b)', 'Pos(r)', 'W(100%)', 'H(100%)', 'Ov(h)', 'V(h)', 'active_V(v)']\n",
      "陸疫情爆不停 逾60官被開鍘\n",
      "['D(b)', 'Pos(r)', 'W(100%)', 'H(100%)', 'Ov(h)', 'V(h)', 'active_V(v)']\n",
      "最新！桃機大跳電 2人遭逮\n",
      "['Fz(16px)', 'LineClamp(1,20px)', 'Fw(700)', 'Td(n)', 'Td(u):h', 'C(#324fe1)', 'V(h)', 'active_V(v)']\n",
      "徐國勇挺役期延長：1年算什麼\n",
      "['Fz(16px)', 'LineClamp(1,20px)', 'Fw(700)', 'Td(n)', 'Td(u):h', 'C(#324fe1)', 'V(h)', 'active_V(v)']\n",
      "房仲：租金漲幅超過官方想像\n",
      "['Fz(16px)', 'LineClamp(1,20px)', 'Fw(700)', 'Td(n)', 'Td(u):h', 'C(#324fe1)', 'V(h)', 'active_V(v)']\n",
      "電子釀災 台股重挫逾300點\n",
      "['Fz(16px)', 'LineClamp(1,20px)', 'Fw(700)', 'Td(n)', 'Td(u):h', 'C(#324fe1)', 'V(h)', 'active_V(v)']\n",
      "3億人陷危機 戰爭再砸「飯碗」\n",
      "['Fz(16px)', 'LineClamp(1,20px)', 'Fw(700)', 'Td(n)', 'Td(u):h', 'C(#324fe1)', 'V(h)', 'active_V(v)']\n"
     ]
    }
   ],
   "source": [
    "# 使用爬蟲列出Yahoo焦點新聞 這類使用find_all直接找a標籤和get('href')更快\n",
    "import bs4,requests\n",
    "\n",
    "url='https://tw.yahoo.com/'\n",
    "html=requests.get(url)\n",
    "obj=bs4.BeautifulSoup(html.text,'lxml')\n",
    "tag_1=obj.select('.panels #panel0-content')\n",
    "news=tag_1[0].find_all('a')\n",
    "print(len(news))\n",
    "for new in news:\n",
    "    print(new.text)  # 抓到文字內容\n",
    "    print(new.get('href'))\n",
    "    # print(new.get('class')) # get()就是抓到標籤內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf8213e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"ball_tx ball_green\">08 </div>,\n",
       " <div class=\"ball_tx ball_green\">15 </div>,\n",
       " <div class=\"ball_tx ball_green\">12 </div>,\n",
       " <div class=\"ball_tx ball_green\">04 </div>,\n",
       " <div class=\"ball_tx ball_green\">14 </div>,\n",
       " <div class=\"ball_tx ball_green\">17 </div>,\n",
       " <div class=\"ball_tx ball_green\">04 </div>,\n",
       " <div class=\"ball_tx ball_green\">08 </div>,\n",
       " <div class=\"ball_tx ball_green\">12 </div>,\n",
       " <div class=\"ball_tx ball_green\">14 </div>,\n",
       " <div class=\"ball_tx ball_green\">15 </div>,\n",
       " <div class=\"ball_tx ball_green\">17 </div>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用爬蟲找出威力採最新開獎結果 練習用find_all一下\n",
    "import bs4,requests\n",
    "\n",
    "ball_list=[]\n",
    "url='http://www.taiwanlottery.com.tw'\n",
    "html=requests.get(url)\n",
    "html.raise_for_status()\n",
    "obj=bs4.BeautifulSoup(html.text,'lxml')\n",
    "tag_1=obj.select('.contents_box02')\n",
    "ff=tag_1[0].find_all('div',{'class':'ball_green'})\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "25bd89b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your IP address: 101.12.51.121\n"
     ]
    }
   ],
   "source": [
    "# 使用爬蟲程式列出自己的IP\n",
    "\n",
    "import bs4,requests\n",
    "\n",
    "headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'}\n",
    "url='http://ip.filefab.com/index.php'\n",
    "\n",
    "html=requests.get(url,headers=headers)\n",
    "obj=bs4.BeautifulSoup(html.text,'lxml')\n",
    "find_ip=obj.find('h1',id='ipd')\n",
    "# strip()用來去除頭尾字符、空白符(包括\\n、\\r、\\t、' '，即：換行、回車、制表符、空格)\n",
    "print(find_ip.text.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
